{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and process outputs\n",
    "\n",
    "The script contain the steps from getting raw outputs from API to processing the outputs to get scores (F1, accuracy, precision, recall, support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is about for loop every fold of sentences in every prompt to get outputs through ChatGPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_1.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_2.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_3.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_4.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_5.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_6.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_7.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_8.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_9.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/24_10.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/25_1.txt\n",
      "Written to ../outputs/output_0/first_time_conv_met_raw/25_2.txt\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for gpt-4-1106-preview in organization org-EQ8T0HNMydsyJpkC1R94ksSQ on tokens per day (TPD): Limit 1500000, Used 1498206, Requested 4863. Please try again in 2m56.774s. Visit https://platform.openai.com/account/rate-limits to learn more.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-21dab3eaa8a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmodified_prompt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprompt_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'0 shot - conventional metaphor '\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{text}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Call the OpenAI API to get a response for each sentence group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         response = openai.ChatCompletion.create(\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-4-1106-preview\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"role\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"user\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodified_prompt\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Ana\\Anaconda\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Ana\\Anaconda\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Ana\\Anaconda\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         )\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Ana\\Anaconda\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             return (\n\u001b[1;32m--> 700\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\Ana\\Anaconda\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for gpt-4-1106-preview in organization org-EQ8T0HNMydsyJpkC1R94ksSQ on tokens per day (TPD): Limit 1500000, Used 1498206, Requested 4863. Please try again in 2m56.774s. Visit https://platform.openai.com/account/rate-limits to learn more."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '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'\n",
    "\n",
    "# Read the sentence_forAPI.csv file\n",
    "sentence_df = pd.read_csv('../data/corpus/sen4prompting.csv')\n",
    "\n",
    "# Read the Excel file's \"0-shot\" column\n",
    "prompts_df = pd.read_csv('../data/prompt_set/25_Prompts_Cov_met_sen.csv', usecols=['0 shot - conventional metaphor '])\n",
    "\n",
    "# Iterate through each prompt and each sentence from sentence_forAPI.csv\n",
    "for prompt_index, prompt_row in prompts_df.iloc[23:].iterrows():\n",
    "    for sentence_group_index, sentence_group in enumerate(sentence_df['query_sentence']):\n",
    "        # Replace {text} in the prompt with the sentence group\n",
    "        modified_prompt = prompt_row['0 shot - conventional metaphor '].replace(\"{text}\", sentence_group)\n",
    "        # Call the OpenAI API to get a response for each sentence group\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"user\", \"content\": modified_prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        # Extract the generated text\n",
    "        generated_text = response['choices'][0]['message']['content'] if response['choices'] else 'No response'\n",
    "\n",
    "        # Filename convention \"promptIndex_sentenceGroupIndex.txt\", e.g., \"1_1.txt\"\n",
    "        filename = f\"../outputs/output_0/first_time_conv_met_raw/{prompt_index + 1}_{sentence_group_index + 1}.txt\"\n",
    "        \n",
    "        # Write the generated text to a text file\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(generated_text)\n",
    "        \n",
    "        # Print information to track progress\n",
    "        print(f\"Written to {filename}\")\n",
    "\n",
    "print(\"All results have been written to text files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step involves processing all separate output files into a single formatï¼šAssign digital serial number to every word, and for the first word of the sentence, its digital serial number is always 1; there is blank row between sentences (The blank rows exist when output from API). So I can remove the blank rows and turn the txt into a three column csv (word order, word, label). \n",
    "\n",
    "Content in the txt looks like:\n",
    "\n",
    "1.Hello\n",
    "\n",
    "2.World\n",
    "\n",
    "1.Time\n",
    "\n",
    "2.flies:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_1_1.txt', '1_1_2.txt']\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path where the text files are located\n",
    "directory_path = '../outputs/first_time_running_raw'  # Replace with your directory path\n",
    "\n",
    "# Function to process the text files\n",
    "def process_text_files(directory):\n",
    "    # Get a list of text files in the specified directory\n",
    "    files = [file for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "    \n",
    "    # Process each file\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            # Read the lines in the file\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Initialize variables\n",
    "        new_lines = []\n",
    "        line_number = 1  # Start numbering from 1\n",
    "        \n",
    "        # Process each line\n",
    "        for line in lines:\n",
    "            # Strip out leading numbers and whitespace\n",
    "            clean_line = line.lstrip('0123456789. \\t')\n",
    "            if clean_line.strip():  # If the line is not empty after stripping\n",
    "                # Add a new line number and the cleaned line\n",
    "                new_lines.append(f\"{line_number}. {clean_line}\")\n",
    "                line_number += 1  # Increment the line number\n",
    "            else:\n",
    "                # Reset the line number after an empty line and add the empty line\n",
    "                line_number = 1\n",
    "                new_lines.append('\\n')  # Keep the empty lines\n",
    "        \n",
    "        # Write the processed lines back to the file\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.writelines(new_lines)\n",
    "\n",
    "# Call the function to process the text files\n",
    "process_text_files(directory_path)\n",
    "\n",
    "# Print out the list of files processed for confirmation\n",
    "print(os.listdir(directory_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a csv of three columns, since the sentence order are random, we need to put it back to the order in the manual corpus to make comparison. \n",
    "\n",
    "1.The words are conbined back to sentences based on their word order.\n",
    "\n",
    "2.Get the corresponding sentence id. \n",
    "\n",
    "3.Get the sentence id order in the original corpus.\n",
    "\n",
    "4.Sort out the results based on the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text file created: ../outputs/full_0_1.txt\n",
      "CSV file updated with sentence_id: ../outputs/full_0_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the directory paths and file names\n",
    "directory_path = '../outputs/output_0/first_time_running_raw'  # Replace with your directory path\n",
    "combined_file_path = '../outputs/sortout_outputs/full_sort_raw/full_0_1.txt'  # Path for the combined text file\n",
    "csv_file_path = '../outputs/sortout_outputs/full_0_1.csv'  # Path for the updated csv file\n",
    "corpus_csv_path = '../data/corpus/Corpus_identification_con_met.csv'  # Path to the corpus csv file\n",
    "\n",
    "# Read the corpus CSV to build a mapping of context to sentence_id\n",
    "context_to_sentence_id = {}\n",
    "with open(corpus_csv_path, 'r', encoding='utf-8') as corpus_file:\n",
    "    reader = csv.DictReader(corpus_file)\n",
    "    for row in reader:\n",
    "        context_to_sentence_id[row['context']] = row['sentence_id']\n",
    "\n",
    "# Function to process the text files and update the csv with sentence_id\n",
    "def process_text_files_to_combined_and_csv(directory, combined_file_path, csv_file_path):\n",
    "        # Initialize variables to store all words, numbers, and contexts for the csv\n",
    "    sentences = []  # This will store the list of sentences\n",
    "    current_sentence = []  # This will store the words to form the current sentence\n",
    "    combined_content = []  # This will store the combined content for the .txt file\n",
    "/\n",
    "    # Process each file\n",
    "    for file_name in sorted(os.listdir(directory)):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                # Remove original line numbers and trim whitespace\n",
    "                clean_line = line.lstrip('0123456789. \\t\\n')\n",
    "                if clean_line:  # If line is not empty\n",
    "                    # Split the line at the colon if it exists\n",
    "                    parts = clean_line.split(':')\n",
    "                    word = parts[0].strip()\n",
    "                    number = parts[1].strip() if len(parts) > 1 else ''\n",
    "                    current_sentence.append((word, number))  # Append the word and number to the current sentence\n",
    "                else:\n",
    "                    if current_sentence:  # If there's a sentence to save\n",
    "                        sentences.append(current_sentence)\n",
    "                        current_sentence = []  # Start a new sentence\n",
    "    \n",
    "    # Add the last sentence if it wasn't followed by an empty line\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    # Write the combined .txt file\n",
    "    with open(combined_file_path, 'w', newline='', encoding='utf-8') as txt_file:\n",
    "        for sentence in sentences:\n",
    "            for i, (word, number) in enumerate(sentence, start=1):\n",
    "                combined_content.append(f\"{i}. {word}\\n\")  # Add to the combined content\n",
    "        txt_file.writelines(combined_content)  # Write combined content to the .txt file\n",
    "    # Write the updated CSV file with sentence_id\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Sequence', 'Word', 'Number', 'Context', 'sentence_id'])  # Add new header for sentence_id\n",
    "        rows_to_sort = []  # List to hold rows for sorting\n",
    "        for sentence in sentences:\n",
    "            context = ' '.join([word for word, num in sentence])  # Construct the context\n",
    "            sentence_id = context_to_sentence_id.get(context, 'Not Found')  # Get the sentence_id, default to 'Not Found'\n",
    "            for i, (word, number) in enumerate(sentence, start=1):\n",
    "                rows_to_sort.append([i, word, number or '0', context, sentence_id])  # Append row with sentence_id\n",
    "\n",
    "        # Sort rows by sentence_id as per the order in the corpus csv\n",
    "        rows_to_sort.sort(key=lambda x: int(x[4]) if x[4].isdigit() else x[4])  # Assuming sentence_id is numeric\n",
    "        # Write sorted rows to the csv\n",
    "        writer.writerows(rows_to_sort)\n",
    "\n",
    "# Call the function\n",
    "process_text_files_to_combined_and_csv(directory_path, combined_file_path, csv_file_path)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Combined text file created: {combined_file_path}\")\n",
    "print(f\"CSV file updated with sentence_id: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation part to be finished:\n",
    " \n",
    "1. Post check: if the auto processing output has tokenization problem so the length is different from ground truth in the corpus.\n",
    "\n",
    "2. Filter out the deliberate metaphorical words.\n",
    "\n",
    "3. Calculate the scores.For example,\n",
    "\n",
    "ground_truth = csv_data['maunal annotation']\n",
    "model_output = csv_data['model_output']\n",
    "\n",
    "def calculate_metrics(ground_truth, model_output):\n",
    "\n",
    "    return {\n",
    "        'F1 Score': f1_score(ground_truth, model_output),\n",
    "        'Accuracy': accuracy_score(ground_truth, model_output),\n",
    "        'Recall': recall_score(ground_truth, model_output),\n",
    "        'Precision': precision_score(ground_truth, model_output),\n",
    "        'Support': classification_report(ground_truth, model_output, output_dict=True)['1']['support']\n",
    "    }\n",
    "\n",
    "metrics_results = calculate_metrics(ground_truth, model_output)\n",
    "\n",
    "4. Store the scores in a csv.\n",
    "\n",
    "5. Evaluation: permutation test: whether there is significant difference in comparison of the scores of the same prompts running multiple times, and comparison of the average scores of different prompts; Confidence interval of (accuracy, F1...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
