{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step evaluation scores of the model outputs will be calculated, which includes\n",
    "\n",
    "Accuracy\n",
    "\n",
    "F1 score\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "True positive\n",
    "\n",
    "True negative\n",
    "\n",
    "False positive\n",
    "\n",
    "False negative\n",
    "\n",
    "Support (whow many word or word pairs altogather)\n",
    "\n",
    "Detected_num (How many conventional metaphors  model has detected)\n",
    "\n",
    "Total_Num (How many conventional metaphors are there in manual annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Load the sorted labels\n",
    "df = pd.read_csv('../outputs/output_0/csv/sorted/(secret)first_time_output_sorted.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Select non-deliberate metaphors (conventional metaphors)\n",
    "#df = df[df['DELMET'] != 2]\n",
    "# For the prompts only aim at detecting metaphors, Use the code below rather than the last code\n",
    "df['DELMET'] = df['DELMET'].replace(2, 1)\n",
    "\n",
    "# automatically detect all the columns starting with 'label_'\n",
    "label_columns = [col for col in df.columns if col.startswith('label_')]\n",
    "\n",
    "# 初始化一个DataFrame来存储结果\n",
    "results = pd.DataFrame(index=[col.replace('label_', 'Label_') for col in label_columns],\n",
    "                       columns=['Accuracy', 'F1', 'Precision', 'Recall', 'Detected_num', 'Total_Num',\n",
    "                                'True_Positive', 'True_Negative', 'False_Positive', 'False_Negative'])\n",
    "\n",
    "# 计算手动注释中的传统隐喻数量\n",
    "delmet_ones_count = df['DELMET'].sum()\n",
    "\n",
    "# 计算每个标签列的指标\n",
    "for label_col in label_columns:\n",
    "    y_true = df['DELMET']\n",
    "    y_pred = df[label_col]\n",
    "    \n",
    "    # 计算性能指标\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # 计算预测为正类的数量\n",
    "    detected_num = y_pred.sum()\n",
    "    \n",
    "    # 打印或存储每个标签的结果\n",
    "    print(f\"Label: {label_col}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Detected Positives: {detected_num}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 将结果添加到DataFrame\n",
    "    results.loc[label_col.replace('label_', 'Label_')] = [accuracy, f1, precision, recall, detected_num, delmet_ones_count,\n",
    "                                                          tp, tn, fp, fn]\n",
    "\n",
    "# 将结果保存到新的csv文件\n",
    "results.to_csv('../outputs/output_0/csv/results/(secret)seperate_multiple_time_results/(secret)first_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is an adpted version of the code above, just add two more evalutaion columns:\n",
    "\n",
    "NVAJ_total (How many tokens of noun, verb and adjective in manual annotaion)\n",
    "\n",
    "NVAJ_detected (How many tokens of noun, verb and adjective in manual annotaion are detected by model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv('../outputs/output_0/csv/sorted/(lexiNVAJ)first_time_output_sorted.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Select non-deliberate metaphors (conventional metaphors)\n",
    "df = df[df['DELMET'] != 2]\n",
    "# For the prompts only aim at detecting metaphors, Use the code below rather than the last code\n",
    "#df['DELMET'] = df['DELMET'].replace(2, 1)\n",
    "\n",
    "# Filter rows in wordcat column that do not belong to noun, verb, adjective\n",
    "excluded_tags = ['CJ', 'EX', 'AV', 'AT', 'PN', 'PR', 'DP', 'DT', 'TO', 'XX', 'CR', 'OR', 'UN', 'ZZ']\n",
    "wordcat_filtered = df[~df['wordcat'].isin(excluded_tags)]\n",
    "\n",
    "# 自动检测所有以'label_'开头的列\n",
    "label_columns = [col for col in df.columns if col.startswith('label_')]\n",
    "\n",
    "# 初始化一个DataFrame来存储结果\n",
    "results = pd.DataFrame(index=[col.replace('label_', 'Label_') for col in label_columns],\n",
    "                       columns=['Accuracy', 'F1', 'Precision', 'Recall', ' NVAJ_detected', 'NVAJ_total', 'Detected_num', 'Total_Num',\n",
    "                                'True_Positive', 'True_Negative', 'False_Positive', 'False_Negative'])\n",
    "\n",
    "# 计算手动注释中的传统隐喻数量\n",
    "delmet_ones_count = df['DELMET'].sum()\n",
    "\n",
    "# 计算每个标签列的指标\n",
    "for label_col in label_columns:\n",
    "    y_true = df['DELMET']\n",
    "    y_pred = df[label_col]\n",
    "    \n",
    "    # 计算性能指标\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # 计算预测为正类的数量\n",
    "    detected_num = y_pred.sum()\n",
    "    \n",
    "    # Calculate wordcat related statistics\n",
    "    NVAJ_total = len(wordcat_filtered)\n",
    "    NVAJ_detected = wordcat_filtered[wordcat_filtered['DELMET'] == 1][label_col].sum()\n",
    "    \n",
    "    # 打印或存储每个标签的结果\n",
    "    print(f\"Label: {label_col}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Detected Positives: {detected_num}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 将结果添加到DataFrame\n",
    "    results.loc[label_col.replace('label_', 'Label_')] = [accuracy, f1, precision, recall, NVAJ_detected, NVAJ_total, detected_num, delmet_ones_count,\n",
    "                                                          tp, tn, fp, fn]\n",
    "\n",
    "# 将结果保存到新的csv文件\n",
    "results.to_csv('../outputs/output_0/csv/results/(lexiNVAJ)seperate_multiple_time_results/(lexiNVAJ)first_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "def process_files(fn_pattern):\n",
    "    for file_path in glob(fn_pattern):\n",
    "        df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "        # 过滤掉GT列值为2的行\n",
    "        df = df[df['DELMET'] != 2]\n",
    "\n",
    "        # 找到所有以\"label_\"开头的列\n",
    "        pred_cols = [col for col in df.columns if col.startswith('label_')]\n",
    "        \n",
    "        for pred_col in pred_cols:\n",
    "            gt_l = list(df['DELMET'])  # 真实标签列\n",
    "            pred_l = list(df[pred_col])  # 预测标签列\n",
    "\n",
    "            # 计算混淆矩阵和分类报告\n",
    "            tn, fp, fn, tp = confusion_matrix(gt_l, pred_l).ravel()\n",
    "            report = classification_report(gt_l, pred_l)\n",
    "\n",
    "            # 打印报告和混淆矩阵结果\n",
    "            print(f\"File: {file_path}, Label Column: {pred_col}\")\n",
    "            print(f\"True Positives: {tp}, True Negatives: {tn}, False Positives: {fp}, False Negatives: {fn}\")\n",
    "            print(report)\n",
    "            print(\"\\n\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #pattern = \"(conNVAJ)*_output_sorted.csv\"  # 修改这个模式以匹配你的文件\n",
    "    #main(pattern)\n",
    "    # 设置文件路径模式\n",
    "file_pattern = '../outputs/output_0/csv/sorted/(conventional)*_output_sorted.csv'  # 请替换为你的文件路径\n",
    "\n",
    "# 调用函数处理文件\n",
    "process_files(file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "\n",
    "def main(fn): \n",
    "gt_l=[]\n",
    "pred_l=[]\n",
    "for f in glob(fn):\n",
    "df = pd.read_csv(f)\n",
    "gt_l+=list(df['GT’]) # Adapt to your own column names\n",
    "pred_l+=list(df['PRED’]) # Idem\n",
    "report=classification_report(gt_l,pred_l)\n",
    "print(report)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "main(sys.argv[1]) # a regex referring to all prediction files for one metaphor type, like \"conventional_*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model has deviations in each output result under the same prompt, the experiment was repeated three times. The following code is used to calculate the average of the three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all all the multiple times results\n",
    "file_names = [\n",
    "    '../outputs/output_0/csv/results/(lexiNVAJ)seperate_multiple_time_results/(lexiNVAJ)third_results.csv', \n",
    "    '../outputs/output_0/csv/results/(lexiNVAJ)seperate_multiple_time_results/(lexiNVAJ)second_results.csv', \n",
    "    '../outputs/output_0/csv/results/(lexiNVAJ)seperate_multiple_time_results/(lexiNVAJ)first_results.csv'\n",
    "]\n",
    "dataframes = [pd.read_csv(file_name, index_col=0) for file_name in file_names]\n",
    "\n",
    "# Detect how many Label_i columns there are in each DataFrame\n",
    "label_columns = [col for col in dataframes[0].columns if col.startswith('Label_')]\n",
    "\n",
    "# Calculate the average of the metrics for these datasets at each cue\n",
    "mean_df = pd.concat(dataframes).groupby(level=0).mean()\n",
    "\n",
    "# Save average results to CSV file\n",
    "mean_df.to_csv('../outputs/output_0/csv/results/overall_results/(lexiNVAJ)average_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further evaluation. Details to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct Shapiro-Wilk test for accuracy, f1, precision, recall for each prompt，and choose appropriate test\n",
    "test_results = {}\n",
    "\n",
    "for label in dataframes[0].index:  # The index of all the DataFrames are same\n",
    "    test_results[label] = {}\n",
    "    for column in dataframes[0].columns:  #  The column name for all the DataFrames are same\n",
    "        # get the data for all the prompts of from the mltiple times reults\n",
    "        group_values = [df.loc[label, column] for df in dataframes]\n",
    "        \n",
    "        # Conduct Shapiro-Wilk test\n",
    "        shapiro_test = stats.shapiro(group_values)\n",
    "        if shapiro_test.pvalue > 0.05:\n",
    "            # if normall y distributed，conduct ANOVA\n",
    "            f_value, p_value = stats.f_oneway(*[df[column] for df in dataframes])\n",
    "            test_results[label][column] = ('ANOVA', f_value, p_value)\n",
    "        else:\n",
    "            # if not normall y distributed，conduct Kruskal-Wallis\n",
    "            h_value, p_value = stats.kruskal(*[df[column] for df in dataframes])\n",
    "            test_results[label][column] = ('Kruskal-Wallis', h_value, p_value)\n",
    "\n",
    "# output\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for label, metrics in test_results.items():\n",
    "    for metric, values in metrics.items():\n",
    "        if metric != 'Support':  # 排除 'Support' 指标\n",
    "            test_name, statistic, p_value = values\n",
    "            rows.append([label, metric, test_name, statistic, p_value])\n",
    "\n",
    "df_no_support = pd.DataFrame(rows, columns=['Label', 'Metric', 'Test', 'Statistic', 'P-Value'])\n",
    "df_no_support.to_csv('../outputs/output_0/csv/results/overall_results/[metaphor]multiple_time_result_significance_comparison.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation part to be finished:\n",
    " \n",
    "1. Post check: if the auto processing output has tokenization problem so the length is different from ground truth in the corpus.\n",
    "\n",
    "2. Filter out the deliberate metaphorical words.\n",
    "\n",
    "3. Calculate the scores.For example,\n",
    "\n",
    "ground_truth = csv_data['maunal annotation']\n",
    "model_output = csv_data['model_output']\n",
    "\n",
    "def calculate_metrics(ground_truth, model_output):\n",
    "\n",
    "    return {\n",
    "        'F1 Score': f1_score(ground_truth, model_output),\n",
    "        'Accuracy': accuracy_score(ground_truth, model_output),\n",
    "        'Recall': recall_score(ground_truth, model_output),\n",
    "        'Precision': precision_score(ground_truth, model_output),\n",
    "        'Support': classification_report(ground_truth, model_output, output_dict=True)['1']['support']\n",
    "    }\n",
    "\n",
    "metrics_results = calculate_metrics(ground_truth, model_output)\n",
    "\n",
    "4. Store the scores in a csv.\n",
    "\n",
    "5. Evaluation: permutation test: whether there is significant difference in comparison of the scores of the same prompts running multiple times, and comparison of the average scores of different prompts; Confidence interval of (accuracy, F1...)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
