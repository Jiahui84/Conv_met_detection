{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This script will output 4 files altogether:\n",
    "\n",
    "1.Corpus_identification_con_met.csv: Modified conventional metaphor corpus from deliberate metaphor corpus.\n",
    "\n",
    "2.sentence_overview.csv: Showing the distribution the sentence by random selection (percentage of train set and test set; examples; 10-fold).\n",
    "\n",
    "3.sen4prompting.csv: sentences sorted in 10 groups used for labelling.\n",
    "\n",
    "4.example4prompting.csv: example sentences and their word list with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\ana\\anaconda\\lib\\site-packages (2.0.1)\n",
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/d9/41/23d0ff53cf1fe687b1cd685bbf25785d6a5326ad4394a5855f8cdff33481/pyreadstat-1.2.4-cp38-cp38-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.4-cp38-cp38-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\ana\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Downloading pyreadstat-1.2.4-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: orange3-imageanalytics 0.6.0 has a non-standard dependency specifier numpy>=1.16.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of orange3-imageanalytics or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: colab 1.13.5 has a non-standard dependency specifier pytz>=2011n. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of colab or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Use the installation if there is pyreadstat, pandas or any other library missing\n",
    "#pip install pyreadstat\n",
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covert to conventional metaphor corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conventional metaphor corpus will be adapted from the deliberate metaphor corpus, which is a spss file in sav format. The processing will include steps:\n",
    "\n",
    "1.Make sure every word pair is connected with \"+\" and there is no space between words. For example, \"I find+out the L-shaped desk\". So in the later stage ChatGPT can just tokenize sentence by space (same tokenization way as that in the corpus).\n",
    "\n",
    "2.The corpus consists of academic articles, converation texts, novels and News texts. Filter out others but News texts.\n",
    "\n",
    "3.Reset the labels, non metaphor:0, conventional metaphor (non-deliberate metaphor):1, deliberate metaphor (activated conventional metaphor and novel metaphor, the original label):2.\n",
    "\n",
    "4.Conbine words into sentence again.\n",
    "\n",
    "5.Create sentence is based on docid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure machine will always tokenize the text in the way we want\n",
    "def join_multiple_words(word):\n",
    "    # Ensure that word is a string\n",
    "    word = str(word)\n",
    "    # Use regular expressions to remove spaces around plus or minus signs\n",
    "    word = re.sub(r\"\\s*\\+\\s*\", \"+\", word)\n",
    "    word = re.sub(r\"\\s*-\\s*\", \"-\", word)\n",
    "    words = word.split()\n",
    "    if len(words) > 1:\n",
    "        return '+'.join(words)\n",
    "    return word\n",
    "\n",
    "# Read spss format deliberate metaphor corpus\n",
    "df, meta = pyreadstat.read_sav('../data/corpus/Corpus_identification_del_met.sav')\n",
    "\n",
    "# Filter the labelling of news texts \n",
    "df = df[df['reg'] == 'news']\n",
    "\n",
    "# Replace the label of deliberate metaphor in DELMET column from 1to 2\n",
    "df['DELMET'] = df['DELMET'].replace(1, 2)\n",
    "\n",
    "# Find out the conventional metaphors(non-deliberate metaphors)\n",
    "# When the 'type' column is 'met' and the 'DELMET' column is not 2, fill the cell corresponding to the 'DELMET' column with 1\n",
    "df.loc[(df['type'] == 'met') & (df['DELMET'] != 2), 'DELMET'] = 1\n",
    "\n",
    "# Find out the non metaphors (non-deliberate metaphors)\n",
    "# When the 'type' column is not 'met', fill the cell corresponding to the 'DELMET' column with 1\n",
    "df.loc[(df['type'] != 'met') & (df['DELMET'].isna()), 'DELMET'] = 0\n",
    "\n",
    "# add two columns at the end 'context' and 'sentence_id'\n",
    "df['context'] = None\n",
    "df['sentence_id'] = None\n",
    "\n",
    "# Used to save the current sentence\n",
    "sentence_words = []\n",
    "# Used to save the last docid of the current document\n",
    "last_docid = None\n",
    "# used to track the sentence order number of the current document\n",
    "sentence_number = 1\n",
    "\n",
    "# For loop DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Apply new function to process words of 'word' column\n",
    "    processed_word = join_multiple_words(row['word'])\n",
    "    df.at[index, 'word'] = processed_word  # Make sure to update the DataFrame\n",
    "    \n",
    "    # Check if it is the firt word of a new document or the beginning of a new sentence\n",
    "    if last_docid != row['docid'] or row['windex'] == 1:\n",
    "        # If it is a new document, reset the sentence number\n",
    "        if last_docid != row['docid']:\n",
    "            sentence_number = 1\n",
    "        # Combine the words from the previous sentence into a sentence and populate the context column\n",
    "        if sentence_words:\n",
    "            sentence = ' '.join(sentence_words)\n",
    "            start_index = index - len(sentence_words)\n",
    "            df.loc[start_index:index-1, 'context'] = sentence\n",
    "            df.loc[start_index:index-1, 'sentence_id'] = f\"{last_docid}_{sentence_number}\"\n",
    "            sentence_number += 1\n",
    "            sentence_words = []  # Reset the word list of the current sentence\n",
    "        last_docid = row['docid']\n",
    "    \n",
    "    # Add processed words to current sentence list\n",
    "    sentence_words.append(processed_word)\n",
    "\n",
    "# Populate the context column for the last sentence\n",
    "if sentence_words:\n",
    "    sentence = ' '.join(sentence_words)\n",
    "    start_index = index - len(sentence_words) + 1\n",
    "    df.loc[start_index:index, 'context'] = sentence\n",
    "    df.loc[start_index:index, 'sentence_id'] = f\"{last_docid}_{sentence_number}\"\n",
    "\n",
    "# Save the csv file\n",
    "df.to_csv('../data/corpus/Corpus_identification_con_met.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is about how to distribute the corpus (e.g. percentage of train set and test set; which sentences are used as examples in the prompt for maodel to learn):\n",
    "\n",
    "1.Create a csv with unique sentences.\n",
    "\n",
    "2.Add a column containing labels \"train\" or \"test\", and the percentage is train 10% vs test 90%.Randomly give sentences \"train\" or \"test\" labels.\n",
    "\n",
    "3.Randomly select 10 examples respectively in train set and test set. Examples must not caontain deliberate metaphors.\n",
    "\n",
    "4.Randomly seplit the train set into 10 groups (10-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('../data/corpus/Corpus_identification_con_met.csv')\n",
    "\n",
    "# Select columns \"sentence_id\" and \"context\"\n",
    "df = df[['sentence_id', 'context', 'DELMET']]\n",
    "\n",
    "# label the sentences contain deliberate metaphor(s)\n",
    "df['DEL'] = df.groupby(['sentence_id', 'context'])['DELMET'].transform(lambda x: 2 in x.values)\n",
    "\n",
    "# remove duplicate sentences\n",
    "df = df.drop_duplicates(subset=['context'])\n",
    "\n",
    "#Delete Column DEL\n",
    "df = df.drop(columns=['DELMET'])\n",
    "\n",
    "# Set a random number seed to ensure replication\n",
    "np.random.seed(1)\n",
    "\n",
    "# Add a \"split\" column to randomly assign sentences to train and test\n",
    "df['split'] = np.where(np.random.rand(len(df)) <= 0.1, 'train', 'test')\n",
    "\n",
    "# Add an \"examples\" column and initialize all values to None\n",
    "df['examples'] = None\n",
    "\n",
    "# For train set and test set, 10 sentences are randomly repectively selected and marked as example, and the sentences marked as False in the DEL column are filtered out.\n",
    "train_examples = df[(df['split'] == 'train') & (df['DEL'] == False)].sample(16, random_state=1).index\n",
    "test_examples = df[(df['split'] == 'test') & (df['DEL'] == False)].sample(16, random_state=1).index\n",
    "df.loc[train_examples, 'examples'] = 'example'\n",
    "df.loc[test_examples, 'examples'] = 'example'\n",
    "\n",
    "# df is the DataFrame\n",
    "df['10_fold'] = None\n",
    "\n",
    "# Randomly shuffle data\n",
    "train_sentences = df[(df['split'] == 'train') & (df['examples'].isnull())].sample(frac=1, random_state=1)\n",
    "\n",
    "# Randomly split the data set into 10 folds\n",
    "folds = np.array_split(train_sentences.index, 10)\n",
    "for fold_number, indices in enumerate(folds, 1):\n",
    "    df.loc[indices, '10_fold'] = fold_number\n",
    "\n",
    "# save the csv file\n",
    "df.to_csv('../data/corpus/sentence_overview.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting sentence for input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a sentences in the way that can easily for loop in API: 10 groups of sentences in 10 cells in csv. In each cell, sentences are organized in the format of:\n",
    "\n",
    "1.Sentence 1\n",
    "\n",
    "2.Sentence 2\n",
    "\n",
    "......\n",
    "\n",
    "18.Sentence 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "random.seed(42)  # Specify your desired seed value in the parentheses\n",
    "\n",
    "# Read CSV file into a DataFrame\n",
    "file_path = '../data/corpus/sentence_overview.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a function to add sentence numbers\n",
    "def number_sentences(group):\n",
    "    sentences = []\n",
    "    for s, sid in zip(group['context'].tolist(), group['sentence_id'].tolist()):\n",
    "        sentences.append(f\"{s} (sentence_id: {sid})\")\n",
    "    return \"\\n\".join(sentences)\n",
    "\n",
    "# Group by '10_fold' and apply the numbering function\n",
    "grouped_df = df.groupby('10_fold').apply(number_sentences).reset_index(name='query_sentence')\n",
    "\n",
    "# Add the 'sentence_id' column to the DataFrame\n",
    "sentence_ids = df.groupby('10_fold').apply(lambda x: \"\\n\".join(f\"{sid}\" for sid in x['sentence_id'].tolist())).tolist()\n",
    "grouped_df['sentence_id'] = sentence_ids\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "grouped_df.to_csv('../data/corpus/sen4prompting.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting eamples for N-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples require sentence and corresponding word labelling.This step is about how to get them and store them in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "sentence_overview_df = pd.read_csv('../data/corpus/sentence_overview.csv')\n",
    "corpus_df = pd.read_csv('../data/corpus/Corpus_identification_con_met.csv')\n",
    "\n",
    "# Filter out rows labelled with \"example\" in \"examples\" column\n",
    "filtered_sentence_overview_df = sentence_overview_df[sentence_overview_df['examples'] == 'example'][['examples', 'split', 'context', 'sentence_id']]\n",
    "\n",
    "# Function to create a word list\n",
    "def create_word_list(context):\n",
    "    relevant_rows = corpus_df[corpus_df['context'] == context]\n",
    "    word_list = [f\"{word}{':1' if delmet else ''}\" for word, delmet in zip(relevant_rows['word'], relevant_rows['DELMET'])]\n",
    "    return \"\\n\".join(word_list)\n",
    "\n",
    "filtered_sentence_overview_df['word_list'] = filtered_sentence_overview_df['context'].apply(create_word_list)\n",
    "\n",
    "# Add a new grouping column\n",
    "def assign_groups(df, split, group_sizes):\n",
    "    split_df = df[df['split'] == split]\n",
    "    for size in group_sizes:\n",
    "        selected_indices = split_df.sample(n=size if len(split_df) >= size else len(split_df)).index\n",
    "        df.loc[selected_indices, 'group'] = size\n",
    "        split_df = split_df.drop(selected_indices)\n",
    "\n",
    "# Separate groups for 'train' and 'test'\n",
    "assign_groups(filtered_sentence_overview_df, 'train', [1, 5, 10])\n",
    "assign_groups(filtered_sentence_overview_df, 'test', [1, 5, 10])\n",
    "\n",
    "# save as a new csv file\n",
    "filtered_sentence_overview_df.to_csv('../data/corpus/example4prompting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
