{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "This script will output 4 files altogether:\n",
    "\n",
    "1.Corpus_identification_con_met.csv: Modified conventional metaphor corpus from deliberate metaphor corpus.\n",
    "\n",
    "2.sentence_overview.csv: Showing the distribution the sentence by random selection (percentage of train set and test set; examples; 10-fold).\n",
    "\n",
    "3.sen4prompting.csv: sentences sorted in 10 groups used for labelling.\n",
    "\n",
    "4.example4prompting.csv: example sentences and their word list with labels.\n",
    "\n",
    "5.selected_permuted_prompts.csv: selection of 25 unique prompts from permutation of the whole prompt set for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\ana\\anaconda\\lib\\site-packages (2.0.1)\n",
      "Collecting pyreadstat\n",
      "  Obtaining dependency information for pyreadstat from https://files.pythonhosted.org/packages/d9/41/23d0ff53cf1fe687b1cd685bbf25785d6a5326ad4394a5855f8cdff33481/pyreadstat-1.2.4-cp38-cp38-win_amd64.whl.metadata\n",
      "  Downloading pyreadstat-1.2.4-cp38-cp38-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\python\\ana\\anaconda\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\ana\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Downloading pyreadstat-1.2.4-cp38-cp38-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyreadstat\n",
      "Successfully installed pyreadstat-1.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: orange3-imageanalytics 0.6.0 has a non-standard dependency specifier numpy>=1.16.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of orange3-imageanalytics or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: celery 5.0.5 has a non-standard dependency specifier pytz>dev. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of celery or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: colab 1.13.5 has a non-standard dependency specifier pytz>=2011n. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of colab or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Use the installation if there is pyreadstat, pandas or any other library missing\n",
    "#pip install pyreadstat\n",
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conventional metaphor corpus will be adapted from the deliberate metaphor corpus, which is a spss file in sav format. The processing will include steps:\n",
    "\n",
    "1.Make sure every word pair are connected with \"+\" and there is no space between words. For example, \"I find+out the L-shaped desk\". So in the later stage ChatGPT can just tokenize sentence by space (same tokenization way as that in the corpus).\n",
    "\n",
    "2.The corpus consists of academic articles, converation texts, novels and News texts. Filter out others but News texts.\n",
    "\n",
    "3.Reset the labels, non metaphor:0, conventional metaphor (non-deliberate metaphor):1, deliberate metaphor (activated conventional metaphor and novel metaphor, the original label):2.\n",
    "\n",
    "4.Conbine words into sentence again.\n",
    "\n",
    "5.Create sentence is based on docid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure machine will always tokenize the text in the way we want\n",
    "def join_multiple_words(word):\n",
    "    # Ensure that word is a string\n",
    "    word = str(word)\n",
    "    # Use regular expressions to remove spaces around plus or minus signs\n",
    "    word = re.sub(r\"\\s*\\+\\s*\", \"+\", word)\n",
    "    word = re.sub(r\"\\s*-\\s*\", \"-\", word)\n",
    "    words = word.split()\n",
    "    if len(words) > 1:\n",
    "        return '+'.join(words)\n",
    "    return word\n",
    "\n",
    "# Read spss format deliberate metaphor corpus\n",
    "df, meta = pyreadstat.read_sav('../data/corpus/Corpus_identification_del_met.sav')\n",
    "\n",
    "# Filter the labelling of news texts \n",
    "df = df[df['reg'] == 'news']\n",
    "\n",
    "# Replace the label of deliberate metaphor in DELMET column from 1to 2\n",
    "df['DELMET'] = df['DELMET'].replace(1, 2)\n",
    "\n",
    "# Find out the conventional metaphors(non-deliberate metaphors)\n",
    "# When the 'type' column is 'met' and the 'DELMET' column is not 2, fill the cell corresponding to the 'DELMET' column with 1\n",
    "df.loc[(df['type'] == 'met') & (df['DELMET'] != 2), 'DELMET'] = 1\n",
    "\n",
    "# Find out the non metaphors (non-deliberate metaphors)\n",
    "# When the 'type' column is not 'met', fill the cell corresponding to the 'DELMET' column with 1\n",
    "df.loc[(df['type'] != 'met') & (df['DELMET'].isna()), 'DELMET'] = 0\n",
    "\n",
    "# add two columns at the end 'context' and 'sentence_id'\n",
    "df['context'] = None\n",
    "df['sentence_id'] = None\n",
    "\n",
    "# Used to save the current sentence\n",
    "sentence_words = []\n",
    "# Used to save the last docid of the current document\n",
    "last_docid = None\n",
    "# used to track the sentence order number of the current document\n",
    "sentence_number = 1\n",
    "\n",
    "# For loop DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Apply new function to process words of 'word' column\n",
    "    processed_word = join_multiple_words(row['word'])\n",
    "    df.at[index, 'word'] = processed_word  # Make sure to update the DataFrame\n",
    "    \n",
    "    # Check if it is the firt word of a new document or the beginning of a new sentence\n",
    "    if last_docid != row['docid'] or row['windex'] == 1:\n",
    "        # If it is a new document, reset the sentence number\n",
    "        if last_docid != row['docid']:\n",
    "            sentence_number = 1\n",
    "        # Combine the words from the previous sentence into a sentence and populate the context column\n",
    "        if sentence_words:\n",
    "            sentence = ' '.join(sentence_words)\n",
    "            start_index = index - len(sentence_words)\n",
    "            df.loc[start_index:index-1, 'context'] = sentence\n",
    "            df.loc[start_index:index-1, 'sentence_id'] = f\"{last_docid}_{sentence_number}\"\n",
    "            sentence_number += 1\n",
    "            sentence_words = []  # Reset the word list of the current sentence\n",
    "        last_docid = row['docid']\n",
    "    \n",
    "    # Add processed words to current sentence list\n",
    "    sentence_words.append(processed_word)\n",
    "\n",
    "# Populate the context column for the last sentence\n",
    "if sentence_words:\n",
    "    sentence = ' '.join(sentence_words)\n",
    "    start_index = index - len(sentence_words) + 1\n",
    "    df.loc[start_index:index, 'context'] = sentence\n",
    "    df.loc[start_index:index, 'sentence_id'] = f\"{last_docid}_{sentence_number}\"\n",
    "\n",
    "# Save the csv file\n",
    "df.to_csv('../data/corpus/Corpus_identification_con_met.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is about how to distribute the corpus (e.g. percentage of train set and test set; which sentences are used as examples in the prompt for maodel to learn):\n",
    "\n",
    "1.Create a csv with unique sentences.\n",
    "\n",
    "2.Add a column containing labels \"train\" or \"test\", and the percentage is train 10% vs test 90%.Randomly give sentences \"train\" or \"test\" labels.\n",
    "\n",
    "3.Randomly select 10 examples respectively in train set and test set. Examples must not caontain deliberate metaphors.\n",
    "\n",
    "4.Randomly seplit the train set into 10 groups (10-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "df = pd.read_csv('../data/corpus/Corpus_identification_con_met.csv')\n",
    "\n",
    "# Select columns \"sentence_id\" and \"context\"\n",
    "df = df[['sentence_id', 'context', 'DELMET']]\n",
    "\n",
    "# label the sentences contain deliberate metaphor(s)\n",
    "df['DEL'] = df.groupby(['sentence_id', 'context'])['DELMET'].transform(lambda x: 2 in x.values)\n",
    "\n",
    "# remove duplicate sentences\n",
    "df = df.drop_duplicates(subset=['context'])\n",
    "\n",
    "#Delete Column DEL\n",
    "df = df.drop(columns=['DELMET'])\n",
    "\n",
    "# Set a random number seed to ensure replication\n",
    "np.random.seed(1)\n",
    "\n",
    "# Add a \"split\" column to randomly assign sentences to train and test\n",
    "df['split'] = np.where(np.random.rand(len(df)) <= 0.1, 'train', 'test')\n",
    "\n",
    "# Add an \"examples\" column and initialize all values to None\n",
    "df['examples'] = None\n",
    "\n",
    "# For train set and test set, 10 sentences are randomly repectively selected and marked as example, and the sentences marked as False in the DEL column are filtered out.\n",
    "train_examples = df[(df['split'] == 'train') & (df['DEL'] == False)].sample(10, random_state=1).index\n",
    "test_examples = df[(df['split'] == 'test') & (df['DEL'] == False)].sample(10, random_state=1).index\n",
    "df.loc[train_examples, 'examples'] = 'example'\n",
    "df.loc[test_examples, 'examples'] = 'example'\n",
    "\n",
    "# df is the DataFrame\n",
    "df['10_fold'] = None\n",
    "\n",
    "# Randomly shuffle data\n",
    "train_sentences = df[(df['split'] == 'train') & (df['examples'].isnull())].sample(frac=1, random_state=1)\n",
    "\n",
    "# Randomly split the data set into 10 folds\n",
    "folds = np.array_split(train_sentences.index, 10)\n",
    "for fold_number, indices in enumerate(folds, 1):\n",
    "    df.loc[indices, '10_fold'] = fold_number\n",
    "\n",
    "# save the csv file\n",
    "df.to_csv('../data/corpus/sentence_overview.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a sentences in the way that can easily for loop in API: 10 groups of sentences in 10 cells in csv. In each cell, sentences are organized in the format of:\n",
    "\n",
    "1.Sentence 1\n",
    "\n",
    "2.Sentence 2\n",
    "\n",
    "......\n",
    "\n",
    "18.Sentence 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_file_path.csv' with the path to your CSV file\n",
    "file_path = '../data/corpus/sentence_overview.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to create numbered sentences\n",
    "def number_sentences(group):\n",
    "    return \"\\n\".join(f\"{i+1}. {s}\" for i, s in enumerate(group['context'].tolist()))\n",
    "\n",
    "# Group by '10-fold' and apply the numbering function\n",
    "grouped_df = df.groupby('10_fold').apply(number_sentences).reset_index(name='query_sentence')\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "grouped_df.to_csv('../data/corpus/sen4prompting.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples require sentence and corresponding word labelling.This step is about how to get them and store them in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is loaded into the sentence_overview_df and corpus_df DataFrames.\n",
    "# Replace 'path_to_sentence_overview_csv' and 'path_to_corpus_identification_csv' with the actual file paths\n",
    "sentence_overview_df = pd.read_csv('../data/corpus/sentence_overview.csv')\n",
    "corpus_df = pd.read_csv('../data/corpus/Corpus_identification_con_met.csv')\n",
    "\n",
    "# Filter out the rows where 'examples' column has the word 'example'\n",
    "filtered_sentence_overview_df = sentence_overview_df[sentence_overview_df['examples'] == 'example'][['examples', 'split', 'context','sentence_id']]\n",
    "\n",
    "# Define a function to create a word list for the given context\n",
    "def create_word_list(context):\n",
    "    # Filter the corpus dataframe for the given context\n",
    "    relevant_rows = corpus_df[corpus_df['context'] == context]\n",
    "    # Format the word list, appending ':1' if DELMET is 1\n",
    "    word_list = [f\"{word}{':1' if delmet else ''}\" for word, delmet in zip(relevant_rows['word'], relevant_rows['DELMET'])]\n",
    "    # Join the list into a string with each word on a new line\n",
    "    return \"\\n\".join(word_list)\n",
    "\n",
    "# Apply the function to the 'context' column to create the 'word_list' column\n",
    "filtered_sentence_overview_df['word_list'] = filtered_sentence_overview_df['context'].apply(create_word_list)\n",
    "\n",
    "# Save the csv file\n",
    "filtered_sentence_overview_df.to_csv('../data/corpus/example4prompting.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is about preparing the prompts that will be used for the experiment:\n",
    "\n",
    "1.Generating Prompt Permutations: we start by permuting prompt IDs.\n",
    "\n",
    "2.Sampling Unique Sets: from the pool of all possible permutations, we draw a sample of 100 permutations. Each permutation consists of 5 prompt IDs, selected without repetition from the shuffled order.\n",
    "\n",
    "3.Selecting Non-repetitive Prompts: ensure that the 25 total prompts selected (5 sets of 5 prompts) are all unique. \n",
    "\n",
    "4.Creating a CSV for the Experiment: the selected prompt IDs are then used to filter the original 25_Prompts_Cov_met.csv file. We extract the complete rows corresponding to the selected IDs and compile them into a new CSV file named selected_permuted_prompts.csv. This file will serve as the source of prompts for subsequent experimental procedures.\n",
    "\n",
    "5.Final Verification: perform a final check to ensure that the selected_permuted_prompts.csv file contains the correct number of unique prompts, and that there are no duplications or omissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    prompt_id                                           Features   \n",
      "0           1                              Bare task description  \\\n",
      "1           2                              Bare task description   \n",
      "2           3                              Bare task description   \n",
      "3           4                              Bare task description   \n",
      "4           5                              Bare task description   \n",
      "5           6                              Bare task description   \n",
      "6           7  Bare task description + instruction steps + fe...   \n",
      "7           8                            background information    \n",
      "8           9        Word classification + bare task description   \n",
      "9          10        Bare task description + word classification   \n",
      "10         11                                          checklist   \n",
      "11         12                            Research Assistant Role   \n",
      "12         13                      Code-Like Parsing Instruction   \n",
      "13         14                                 Collaborative Task   \n",
      "14         15                              Bare task description   \n",
      "15         16                                              steps   \n",
      "16         17                             Background information   \n",
      "17         18                             Background information   \n",
      "18         19                             Background information   \n",
      "19         20  Bbackground information about conventional met...   \n",
      "20         21  Background information about conventional meta...   \n",
      "21         22             Revised from manual annotation process   \n",
      "22         23             Revised from manual annotation process   \n",
      "23         24             Revised from manual annotation process   \n",
      "24         25             Revised from manual annotation process   \n",
      "\n",
      "                                               0 shot   \n",
      "0   It's a conventional metaphor detection task.  ...  \\\n",
      "1   We are focusing on detecting conventional meta...   \n",
      "2   Please process the following text to detect co...   \n",
      "3   Your task is to detect conventional metaphors....   \n",
      "4   Analyze the given text. Only output each word ...   \n",
      "5   For the purpose of metaphor detection,you will...   \n",
      "6   Note: Don’t output step 1 but only output resu...   \n",
      "7   Your mission is to detect metaphors. Remember,...   \n",
      "8   Your task is to identify conventional metaphor...   \n",
      "9   Review the text provided and list out all the ...   \n",
      "10  Using the checklist below, identify convention...   \n",
      "11  Assume the role of a research assistant tasked...   \n",
      "12  Write a 'parser' that processes the following ...   \n",
      "13  Let's work on a conventional metaphor detectio...   \n",
      "14  Your mission is to detect metaphors. Remember,...   \n",
      "15  Your task is to engage in conventional metapho...   \n",
      "16  A conventional metaphor is a figure of speech ...   \n",
      "17  You will be trained to recognize conventional ...   \n",
      "18  Contextual Understanding\\nMetaphors are a key ...   \n",
      "19  Context: Metaphors are figures of speech where...   \n",
      "20  Objective: Identify conventional metaphors wit...   \n",
      "21  Below is a workflow of conventional metaphor d...   \n",
      "22  Role: Conventional Metaphor Detector\\n\\nTask: ...   \n",
      "23  Task: Spot conventional metaphors in the text....   \n",
      "24  \\nTo ensure the model learns from a provided e...   \n",
      "\n",
      "                                             1-n shot   \n",
      "0   It's a conventional metaphor detection task. L...  \\\n",
      "1   We are focusing on detecting conventional meta...   \n",
      "2   Before you process the main text, please refer...   \n",
      "3   Your task is to detect conventional metaphors....   \n",
      "4   Please train on the provided examples, each ta...   \n",
      "5   For the purpose of metaphor detection, you wil...   \n",
      "6   Note: Don’t output step 2 but only output resu...   \n",
      "7   Your mission is to detect metaphors. Remember,...   \n",
      "8   Your task is to identify conventional metaphor...   \n",
      "9   Review the text provided and list out all the ...   \n",
      "10  Using the checklist below, identify convention...   \n",
      "11  Task: Analyze the provided text for metaphor u...   \n",
      "12  Before you process the following text, study t...   \n",
      "13  Task: Let's work on a conventional metaphor de...   \n",
      "14  Your mission is to detect metaphors. Remember,...   \n",
      "15  Your task is to engage in conventional metapho...   \n",
      "16  A conventional metaphor is a figure of speech ...   \n",
      "17  You will be trained to recognize conventional ...   \n",
      "18  Contextual Understanding\\nMetaphors are a key ...   \n",
      "19  Context: Metaphors are figures of speech where...   \n",
      "20  Objective: Identify conventional metaphors wit...   \n",
      "21  Below is a workflow of conventional metaphor d...   \n",
      "22  Role: Conventional Metaphor Detector\\n\\nTraini...   \n",
      "23  Task: Spot conventional metaphors in the text....   \n",
      "24  \\nTo ensure the model learns from a provided e...   \n",
      "\n",
      "                    generation  \n",
      "0                     ChatGPT4  \n",
      "1                     ChatGPT4  \n",
      "2                    ChatGPT44  \n",
      "3                     ChatGPT4  \n",
      "4                     ChatGPT4  \n",
      "5                     ChatGPT4  \n",
      "6              manual+ChatGPT4  \n",
      "7                     ChatGPT4  \n",
      "8   ChatGPT4 + manual revision  \n",
      "9                     ChatGPT4  \n",
      "10             manual+ChatGPT4  \n",
      "11                    ChatGPT4  \n",
      "12                    ChatGPT4  \n",
      "13                    ChatGPT4  \n",
      "14                    ChatGPT4  \n",
      "15                    ChatGPT4  \n",
      "16                    ChatGPT4  \n",
      "17                    ChatGPT4  \n",
      "18                    ChatGPT4  \n",
      "19                    ChatGPT4  \n",
      "20             manual+ChatGPT4  \n",
      "21             manual+ChatGPT4  \n",
      "22                      Manual  \n",
      "23             manual+ChatGPT4  \n",
      "24             manual+ChatGPT4  \n"
     ]
    }
   ],
   "source": [
    "# Load the csv file\n",
    "prompts_df = pd.read_csv('../data/prompt_set/25_Prompts_Cov_met.csv')\n",
    "\n",
    "# Get the 'prompt_id' column\n",
    "prompt_ids = prompts_df['prompt_id'].values\n",
    "\n",
    "# Define the number of permutations and the number of samples you want to take\n",
    "P = 100  # Number of permutations\n",
    "k = 5    # Number of prompt IDs in each sample\n",
    "\n",
    "# Generate P permutations of the prompt IDs\n",
    "permuted_ids = [np.random.permutation(prompt_ids) for _ in range(P)]\n",
    "\n",
    "# Select the first k prompt IDs from each permutation\n",
    "selected_permutations = [perm[:k] for perm in permuted_ids]\n",
    "\n",
    "# Flatten the list of selected permutations to get all unique prompt IDs\n",
    "unique_prompt_ids = np.unique(np.concatenate(selected_permutations))\n",
    "\n",
    "# Filter the DataFrame for the selected prompt IDs\n",
    "selected_prompts_df = prompts_df[prompts_df['prompt_id'].isin(unique_prompt_ids)]\n",
    "\n",
    "# Save the csv file\n",
    "selected_prompts_df.to_csv('../data/prompt_set/selected_permuted_prompts.csv', index=False)\n",
    "\n",
    "# Print out the saved permutations\n",
    "print(selected_prompts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
